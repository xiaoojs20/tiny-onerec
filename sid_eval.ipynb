{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final SID metrics + SID-space visualization\n",
        "\n",
        "完成两件事：\n",
        "\n",
        "1. **最终版 SID 指标计算**（collision / bucket stats / prefix / CUR / entropy / top1_share / PAS + 类别混合度）\n",
        "2. 基于 **SID（离散 token）本身** 的降维可视化：把每个 item 的 SID 当作 4 个离散特征，用 OneHotEncoder → TruncatedSVD 得到 2D 表示，在 2D 平面上用 **类别上色**，看不同品类是否在 SID 空间被分到不同区域。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "SID_JSON_PATH = Path(\"./data/Amazon2018/All_Amazon/SIDs/debug.index.json\")   # SID json\n",
        "EMB_DIR = Path(\"./data/Amazon2018/All_Amazon\")  # embedding 目录（4 个 .npy）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) 读入 embeddings（并自动生成类别标签）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Emb] Loading directory: data/Amazon2018/All_Amazon (files=4)\n",
            "[Emb] rows 0 ~ 8093: Arts_Crafts_and_Sewing.emb-Qwen3-Embedding-4B-td.npy (n=8094, dim=2560)  ->  cat=Arts_Crafts_and_Sewing\n",
            "[Emb] rows 8094 ~ 11526: Industrial_and_Scientific.emb-Qwen3-Embedding-4B-td.npy (n=3433, dim=2560)  ->  cat=Industrial_and_Scientific\n",
            "[Emb] rows 11527 ~ 19595: Office_Products.emb-Qwen3-Embedding-4B-td.npy (n=8069, dim=2560)  ->  cat=Office_Products\n",
            "[Emb] rows 19596 ~ 22740: Video_Games.emb-Qwen3-Embedding-4B-td.npy (n=3145, dim=2560)  ->  cat=Video_Games\n",
            "[Emb] Loaded embeddings shape: (22741, 2560)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(22741,\n",
              " [{'file': 'Arts_Crafts_and_Sewing.emb-Qwen3-Embedding-4B-td.npy',\n",
              "   'cat': 'Arts_Crafts_and_Sewing',\n",
              "   'start': 0,\n",
              "   'end': 8093,\n",
              "   'n': 8094},\n",
              "  {'file': 'Industrial_and_Scientific.emb-Qwen3-Embedding-4B-td.npy',\n",
              "   'cat': 'Industrial_and_Scientific',\n",
              "   'start': 8094,\n",
              "   'end': 11526,\n",
              "   'n': 3433},\n",
              "  {'file': 'Office_Products.emb-Qwen3-Embedding-4B-td.npy',\n",
              "   'cat': 'Office_Products',\n",
              "   'start': 11527,\n",
              "   'end': 19595,\n",
              "   'n': 8069},\n",
              "  {'file': 'Video_Games.emb-Qwen3-Embedding-4B-td.npy',\n",
              "   'cat': 'Video_Games',\n",
              "   'start': 19596,\n",
              "   'end': 22740,\n",
              "   'n': 3145}])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def load_embeddings_dir_with_splits(emb_dir: Path):\n",
        "    \"\"\"读取目录下所有 .npy/.npz 并按文件名排序 stack，返回 X, y, splits。\n",
        "    y 是类别字符串数组；splits 记录每个文件对应的行范围（0-based，含起止）。\"\"\n",
        "\n",
        "    约定：文件名形如 `Arts_Crafts_and_Sewing.emb-xxx.npy`，类别名取第一个 `.emb-` 之前的部分。\n",
        "    \"\"\"\n",
        "    files = sorted(list(emb_dir.glob(\"*.npy\")) + list(emb_dir.glob(\"*.npz\")))\n",
        "    if not files:\n",
        "        raise FileNotFoundError(f\"No .npy/.npz files in: {emb_dir}\")\n",
        "\n",
        "    arrays = []\n",
        "    y = []\n",
        "    splits = []\n",
        "    start = 0\n",
        "    dim = None\n",
        "\n",
        "    print(f\"[Emb] Loading directory: {emb_dir} (files={len(files)})\")\n",
        "    for f in files:\n",
        "        obj = np.load(f, allow_pickle=False)\n",
        "        if isinstance(obj, np.lib.npyio.NpzFile):\n",
        "            keys = list(obj.files)\n",
        "            arr = obj[keys[0]] if len(keys) == 1 else obj[\"arr_0\"]\n",
        "        else:\n",
        "            arr = obj\n",
        "        arr = np.asarray(arr)\n",
        "        if arr.ndim == 1:\n",
        "            arr = arr.reshape(1, -1)\n",
        "        if arr.ndim != 2:\n",
        "            raise ValueError(f\"{f} must be 1D/2D, got {arr.shape}\")\n",
        "        if arr.dtype != np.float32:\n",
        "            arr = arr.astype(np.float32, copy=False)\n",
        "\n",
        "        # 清 NaN/Inf\n",
        "        if np.isnan(arr).any() or np.isinf(arr).any():\n",
        "            np.nan_to_num(arr, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        if dim is None:\n",
        "            dim = arr.shape[1]\n",
        "        elif arr.shape[1] != dim:\n",
        "            raise ValueError(f\"Dim mismatch: expected {dim}, got {arr.shape[1]} in {f.name}\")\n",
        "\n",
        "        n = arr.shape[0]\n",
        "        end = start + n - 1\n",
        "\n",
        "        # 类别名：取 `.emb-` 前的部分；否则用 stem\n",
        "        name = f.name\n",
        "        cat = name.split(\".emb-\")[0] if \".emb-\" in name else f.stem\n",
        "\n",
        "        print(f\"[Emb] rows {start} ~ {end}: {f.name} (n={n}, dim={arr.shape[1]})  ->  cat={cat}\")\n",
        "\n",
        "        arrays.append(arr)\n",
        "        y.extend([cat] * n)\n",
        "        splits.append({\"file\": f.name, \"cat\": cat, \"start\": start, \"end\": end, \"n\": n})\n",
        "        start = end + 1\n",
        "\n",
        "    X = np.concatenate(arrays, axis=0)\n",
        "    y = np.asarray(y)\n",
        "    print(f\"[Emb] Loaded embeddings shape: {X.shape}\")\n",
        "    return X, y, splits\n",
        "\n",
        "X, y_cat, splits = load_embeddings_dir_with_splits(EMB_DIR)\n",
        "N, D = X.shape\n",
        "len(y_cat), splits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) 读入 SID json\n",
        "\n",
        "从 SID json 构建 codes_sem (N,3) + suffix (N,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e67b1eac",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SID] loaded entries: 22741, max key=22740\n",
            "0 ['<a_77>', '<b_50>', '<c_50>'] -> ['a', 'b', 'c'] [77, 50, 50]\n",
            "1 ['<a_188>', '<b_229>', '<c_253>', '<d_1>'] -> ['a', 'b', 'c', 'd'] [188, 229, 253, 1]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "TOKEN_RE = re.compile(r\"^<([a-zA-Z]+)_(\\d+)>$\")\n",
        "\n",
        "def load_sid_json(path: Path):\n",
        "    sid_map = json.loads(path.read_text(encoding=\"utf-8\"))\n",
        "    # keys 可能是字符串数字\n",
        "    # 转成 list[ list[str] ]，长度 N（若缺失会填 None）\n",
        "    max_k = max(int(k) for k in sid_map.keys())\n",
        "    return sid_map, max_k\n",
        "\n",
        "sid_map, max_k = load_sid_json(SID_JSON_PATH)\n",
        "print(f\"[SID] loaded entries: {len(sid_map)}, max key={max_k}\")\n",
        "\n",
        "if max_k + 1 != N:\n",
        "    print(f\"[WARN] SID size ({max_k+1}) != embedding N ({N}). \"\n",
        "          f\"后续会按 min(N, max_k+1) 对齐。\")\n",
        "\n",
        "M = min(N, max_k + 1)\n",
        "\n",
        "def parse_tokens(tokens):\n",
        "    layers = []\n",
        "    ids = []\n",
        "    for t in tokens:\n",
        "        m = TOKEN_RE.match(t)\n",
        "        if not m:\n",
        "            raise ValueError(f\"Bad token format: {t}\")\n",
        "        layers.append(m.group(1))\n",
        "        ids.append(int(m.group(2)))\n",
        "    return layers, ids\n",
        "\n",
        "# 解析前几条看看\n",
        "for k in [\"0\", \"1\"]:\n",
        "    if k in sid_map:\n",
        "        layers, ids = parse_tokens(sid_map[k])\n",
        "        print(k, sid_map[k], \"->\", layers, ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SID] M=22741, kept=22741\n",
            "[SID] missing=0, bad=0, len3=16718, len4=6023\n",
            "[SID] codes3 shape=(22741, 3), suffix shape=(22741,), has_suffix=0.265\n",
            "[SID] layer_names3=['a', 'b', 'c']\n",
            "N2, L_sem = 22741 3\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "\n",
        "TOKEN_RE = re.compile(r\"^<([a-zA-Z]+)_(\\d+)>$\")\n",
        "\n",
        "def parse_token(t: str):\n",
        "    m = TOKEN_RE.match(t)\n",
        "    if not m:\n",
        "        raise ValueError(f\"Bad token format: {t}\")\n",
        "    return m.group(1), int(m.group(2))\n",
        "\n",
        "def build_semantic_codes_and_suffix(sid_map: dict, M: int, X: np.ndarray, y_cat: np.ndarray):\n",
        "    \"\"\"\n",
        "    统一语义 SID = 前三层 (a,b,c)，suffix = 第四层 d（如果存在）\n",
        "    - 支持每条 SID 长度为 3 或 4\n",
        "    - 不再用“第一条的长度”过滤数据\n",
        "    \"\"\"\n",
        "    codes3 = []\n",
        "    suffix = []\n",
        "    has_suffix = []\n",
        "    kept_idx = []\n",
        "\n",
        "    n_missing = 0\n",
        "    n_bad = 0\n",
        "    n_len3 = 0\n",
        "    n_len4 = 0\n",
        "\n",
        "    layer_names3 = None\n",
        "\n",
        "    for i in range(M):\n",
        "        k = str(i)\n",
        "        if k not in sid_map:\n",
        "            n_missing += 1\n",
        "            continue\n",
        "\n",
        "        toks = sid_map[k]\n",
        "        if not isinstance(toks, list) or len(toks) < 3:\n",
        "            n_bad += 1\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            layers = []\n",
        "            ids = []\n",
        "            for t in toks:\n",
        "                l, idx = parse_token(t)\n",
        "                layers.append(l)\n",
        "                ids.append(idx)\n",
        "        except Exception:\n",
        "            n_bad += 1\n",
        "            continue\n",
        "\n",
        "        # 语义部分：前三层\n",
        "        if layer_names3 is None:\n",
        "            layer_names3 = layers[:3]\n",
        "\n",
        "        codes3.append(ids[:3])\n",
        "        kept_idx.append(i)\n",
        "\n",
        "        # suffix：若有第四层就记下来\n",
        "        if len(ids) >= 4:\n",
        "            suffix.append(ids[3])\n",
        "            has_suffix.append(True)\n",
        "            n_len4 += 1\n",
        "        else:\n",
        "            suffix.append(-1)\n",
        "            has_suffix.append(False)\n",
        "            n_len3 += 1\n",
        "\n",
        "    codes3 = np.asarray(codes3, dtype=np.int32)\n",
        "    suffix = np.asarray(suffix, dtype=np.int32)\n",
        "    has_suffix = np.asarray(has_suffix, dtype=bool)\n",
        "    kept_idx = np.asarray(kept_idx, dtype=np.int32)\n",
        "\n",
        "    X_ok = X[:M][kept_idx]\n",
        "    y_ok = y_cat[:M][kept_idx]\n",
        "\n",
        "    print(f\"[SID] M={M}, kept={len(kept_idx)}\")\n",
        "    print(f\"[SID] missing={n_missing}, bad={n_bad}, len3={n_len3}, len4={n_len4}\")\n",
        "    print(f\"[SID] codes3 shape={codes3.shape}, suffix shape={suffix.shape}, has_suffix={has_suffix.mean():.3f}\")\n",
        "    print(f\"[SID] layer_names3={layer_names3}\")\n",
        "\n",
        "    return codes3, suffix, has_suffix, X_ok, y_ok, layer_names3\n",
        "\n",
        "# 使用方式（替换你原来的 build_codes_matrix 调用）\n",
        "codes_sem, suffix_d, has_suffix, X_aligned, y, layer_names3 = build_semantic_codes_and_suffix(\n",
        "    sid_map, M, X, y_cat\n",
        ")\n",
        "N2, L_sem = codes_sem.shape\n",
        "print(\"N2, L_sem =\", N2, L_sem)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e56221de",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) 最终指标计算（含类别混合度）"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28717ee9",
      "metadata": {},
      "source": [
        "### 语义指标只用 codes_sem（前三层）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a623dde",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'N': 22741,\n",
              " 'L_sem': 3,\n",
              " 'K_list': [256, 256, 256],\n",
              " 'collision_rate_sem': 0.16305351567653137,\n",
              " 'unique_rate_sem': 0.8369464843234686,\n",
              " 'max_bucket_sem': 28,\n",
              " 'p95_bucket_sem': 2.0,\n",
              " 'p50_bucket_sem': 1.0,\n",
              " 'mean_bucket_sem': 1.1948195239846582,\n",
              " 'num_buckets_sem': 19033,\n",
              " 'collision_rate_prefix_1': 0.9925684886328657,\n",
              " 'max_bucket_prefix_1': 526,\n",
              " 'collision_rate_prefix_2': 0.5696759157468889,\n",
              " 'max_bucket_prefix_2': 46,\n",
              " 'collision_rate_prefix_3': 0.16305351567653137,\n",
              " 'max_bucket_prefix_3': 28,\n",
              " 'CUR_1': 0.66015625,\n",
              " 'entropy_1': 4.769737214658913,\n",
              " 'perplexity_1': 117.88825858361827,\n",
              " 'top1_share_1': 0.02313002946220483,\n",
              " 'CUR_2': 1.0,\n",
              " 'entropy_2': 5.485639766293805,\n",
              " 'perplexity_2': 241.20320832885255,\n",
              " 'top1_share_2': 0.008486873928147398,\n",
              " 'CUR_3': 1.0,\n",
              " 'entropy_3': 5.485301971313853,\n",
              " 'perplexity_3': 241.12174485568147,\n",
              " 'top1_share_3': 0.008926608328569544,\n",
              " 'PAS_sem': 0.9693664501283891,\n",
              " 'bucket_purity_weighted_sem': 0.9981531155182269,\n",
              " 'bucket_purity_unweighted_sem': 0.9991558521164995}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from collections import Counter, defaultdict\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def entropy_and_perplexity(counter: Counter):\n",
        "    total = sum(counter.values())\n",
        "    if total == 0:\n",
        "        return 0.0, 0.0\n",
        "    H = 0.0\n",
        "    for c in counter.values():\n",
        "        p = c / total\n",
        "        H -= p * math.log(p + 1e-12)\n",
        "    return H, math.exp(H)\n",
        "\n",
        "def bucket_stats_from_counter(cnt: Counter):\n",
        "    arr = np.asarray(list(cnt.values()), dtype=np.int32)\n",
        "    return {\n",
        "        \"max_bucket\": int(arr.max()) if len(arr) else 0,\n",
        "        \"p95_bucket\": float(np.percentile(arr, 95)) if len(arr) else 0.0,\n",
        "        \"p50_bucket\": float(np.percentile(arr, 50)) if len(arr) else 0.0,\n",
        "        \"mean_bucket\": float(arr.mean()) if len(arr) else 0.0,\n",
        "        \"num_buckets\": int(len(arr)),\n",
        "    }\n",
        "\n",
        "def cosine_normalize(X):\n",
        "    X = X.astype(np.float32, copy=False)\n",
        "    return X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "def compute_pas(bucket_keys, X_normed, pairs_per_bucket=20, max_keep_per_bucket=30, seed=2024):\n",
        "    buckets = defaultdict(list)\n",
        "    for i, k in enumerate(bucket_keys):\n",
        "        if len(buckets[k]) < max_keep_per_bucket:\n",
        "            buckets[k].append(i)\n",
        "\n",
        "    rng = np.random.default_rng(seed)\n",
        "    sims = []\n",
        "    for idxs in buckets.values():\n",
        "        m = len(idxs)\n",
        "        if m < 2:\n",
        "            continue\n",
        "        num_pairs = min(pairs_per_bucket, m * (m - 1) // 2)\n",
        "        for _ in range(num_pairs):\n",
        "            a, b = rng.choice(idxs, size=2, replace=False)\n",
        "            sims.append(float(np.dot(X_normed[a], X_normed[b])))\n",
        "    return float(np.mean(sims)) if sims else float(\"nan\")\n",
        "\n",
        "def compute_bucket_purity(bucket_keys, labels):\n",
        "    by = defaultdict(list)\n",
        "    for i, k in enumerate(bucket_keys):\n",
        "        by[k].append(i)\n",
        "\n",
        "    total = len(bucket_keys)\n",
        "    weighted = 0.0\n",
        "    unweighted = 0.0\n",
        "    for idxs in by.values():\n",
        "        labs = labels[idxs]\n",
        "        cnt = Counter(labs.tolist())\n",
        "        m = len(idxs)\n",
        "        p = max(cnt.values()) / m\n",
        "        weighted += p * m\n",
        "        unweighted += p\n",
        "    return {\"purity_weighted\": weighted / total, \"purity_unweighted\": unweighted / len(by)}\n",
        "\n",
        "def compute_semantic_metrics(codes_sem: np.ndarray, X: np.ndarray, y: np.ndarray, K_list=None):\n",
        "    N, L = codes_sem.shape  # L=3\n",
        "\n",
        "    # K_list 可手动指定；否则沿用你之前“至少 256”的策略\n",
        "    if K_list is None:\n",
        "        K_list = []\n",
        "        for l in range(L):\n",
        "            mx = int(codes_sem[:, l].max())\n",
        "            K_list.append(256 if mx <= 256 else (mx + 1))\n",
        "\n",
        "    out = {\"N\": int(N), \"L_sem\": int(L), \"K_list\": [int(k) for k in K_list]}\n",
        "\n",
        "    # semantic SID keys\n",
        "    sem_keys = [tuple(row) for row in codes_sem.tolist()]\n",
        "    cnt = Counter(sem_keys)\n",
        "    uniq = len(cnt)\n",
        "\n",
        "    out[\"collision_rate_sem\"] = (N - uniq) / N\n",
        "    out[\"unique_rate_sem\"] = uniq / N\n",
        "    out.update({f\"{k}_sem\": v for k, v in bucket_stats_from_counter(cnt).items()})\n",
        "\n",
        "    # prefix@1/@2/@3（语义层级）\n",
        "    for d in range(1, L + 1):\n",
        "        pref = [tuple(row[:d]) for row in codes_sem[:, :d].tolist()]\n",
        "        pc = Counter(pref)\n",
        "        out[f\"collision_rate_prefix_{d}\"] = (N - len(pc)) / N\n",
        "        out[f\"max_bucket_prefix_{d}\"] = int(np.max(list(pc.values())))\n",
        "\n",
        "    # per-layer distribution\n",
        "    for l in range(L):\n",
        "        K = int(K_list[l])\n",
        "        c = Counter(codes_sem[:, l].tolist())\n",
        "        used = len(c)\n",
        "        H, ppl = entropy_and_perplexity(c)\n",
        "        out[f\"CUR_{l+1}\"] = used / K\n",
        "        out[f\"entropy_{l+1}\"] = H\n",
        "        out[f\"perplexity_{l+1}\"] = ppl\n",
        "        out[f\"top1_share_{l+1}\"] = max(c.values()) / N\n",
        "\n",
        "    # PAS + purity（以 semantic bucket 为单位）\n",
        "    Xn = cosine_normalize(X)\n",
        "    out[\"PAS_sem\"] = compute_pas(sem_keys, Xn)\n",
        "    pur = compute_bucket_purity(sem_keys, y)\n",
        "    out[\"bucket_purity_weighted_sem\"] = pur[\"purity_weighted\"]\n",
        "    out[\"bucket_purity_unweighted_sem\"] = pur[\"purity_unweighted\"]\n",
        "\n",
        "    return out\n",
        "\n",
        "metrics_sem = compute_semantic_metrics(codes_sem, X_aligned, y)\n",
        "metrics_sem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d6857437",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'suffix_present_rate': 0.26485202937425795,\n",
              " 'ambiguous_item_rate': 0.26485202937425795,\n",
              " 'ambiguous_bucket_rate': 0.12163085167866337,\n",
              " 'max_bucket_sem': 28,\n",
              " 'p95_bucket_sem': 2.0,\n",
              " 'suffix_bad_bucket_rate': 0.0,\n",
              " 'checked_ambiguous_buckets': 2315}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def compute_disambiguation_metrics(codes_sem: np.ndarray, suffix_d: np.ndarray, has_suffix: np.ndarray):\n",
        "    \"\"\"\n",
        "    评估第四层“消歧”是否合理 & 消歧成本\n",
        "    \"\"\"\n",
        "    N = codes_sem.shape[0]\n",
        "    sem_keys = [tuple(row) for row in codes_sem.tolist()]\n",
        "\n",
        "    # semantic bucket -> indices\n",
        "    by = defaultdict(list)\n",
        "    for i, k in enumerate(sem_keys):\n",
        "        by[k].append(i)\n",
        "\n",
        "    # 桶大小分布\n",
        "    sizes = np.asarray([len(v) for v in by.values()], dtype=np.int32)\n",
        "\n",
        "    # 需要消歧的 item/桶比例\n",
        "    ambiguous_buckets = sum(1 for v in by.values() if len(v) > 1)\n",
        "    ambiguous_items = sum(len(v) for v in by.values() if len(v) > 1)\n",
        "\n",
        "    # suffix 是否在桶内足够“唯一”\n",
        "    bad_suffix_buckets = 0\n",
        "    checked_buckets = 0\n",
        "    for idxs in by.values():\n",
        "        if len(idxs) <= 1:\n",
        "            continue\n",
        "        checked_buckets += 1\n",
        "        ds = suffix_d[idxs]\n",
        "        hs = has_suffix[idxs]\n",
        "        # 只检查那些“确实给了 suffix”的样本\n",
        "        ds = ds[hs]\n",
        "        if len(ds) == 0:\n",
        "            bad_suffix_buckets += 1\n",
        "        else:\n",
        "            if len(np.unique(ds)) != len(ds):\n",
        "                bad_suffix_buckets += 1\n",
        "\n",
        "    return {\n",
        "        \"suffix_present_rate\": float(has_suffix.mean()),\n",
        "        \"ambiguous_item_rate\": float(ambiguous_items / N),\n",
        "        \"ambiguous_bucket_rate\": float(ambiguous_buckets / len(by)),\n",
        "        \"max_bucket_sem\": int(sizes.max()) if len(sizes) else 0,\n",
        "        \"p95_bucket_sem\": float(np.percentile(sizes, 95)) if len(sizes) else 0.0,\n",
        "        \"suffix_bad_bucket_rate\": float(bad_suffix_buckets / max(1, checked_buckets)),\n",
        "        \"checked_ambiguous_buckets\": int(checked_buckets),\n",
        "    }\n",
        "\n",
        "metrics_disamb = compute_disambiguation_metrics(codes_sem, suffix_d, has_suffix)\n",
        "metrics_disamb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num collided semantic SIDs: 2315\n",
            "top 10 collided buckets:\n",
            "(181, 196, 63) 28\n",
            "(129, 52, 166) 23\n",
            "(218, 200, 166) 23\n",
            "(129, 51, 189) 21\n",
            "(129, 34, 63) 20\n",
            "(122, 67, 97) 16\n",
            "(131, 232, 247) 15\n",
            "(178, 62, 189) 14\n",
            "(213, 225, 40) 14\n",
            "(196, 61, 19) 14\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "sem_keys = [tuple(row) for row in codes_sem.tolist()]\n",
        "cnt = Counter(sem_keys)\n",
        "dups = [(k, v) for k, v in cnt.items() if v > 1]\n",
        "dups.sort(key=lambda x: -x[1])\n",
        "\n",
        "print(\"num collided semantic SIDs:\", len(dups))\n",
        "print(\"top 10 collided buckets:\")\n",
        "for k, v in dups[:10]:\n",
        "    print(k, v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: /mnt/sdb1/sdb1_xiaojinsong/tiny-onerec/sid_metrics.json\n"
          ]
        }
      ],
      "source": [
        "# 保存最终 metrics 到 json\n",
        "OUT_JSON = Path(\"sid_metrics.json\")\n",
        "OUT_JSON.write_text(json.dumps(metrics_sem, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "print(\"Saved:\", OUT_JSON.resolve())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "MiniOneRec",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
