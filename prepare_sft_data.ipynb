{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98e35ca7",
   "metadata": {},
   "source": [
    "# 构造 `LlamaFactory` 可用的 SFT 数据（Alpaca JSONL）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ed906",
   "metadata": {},
   "source": [
    "训练时用到的 5 个 Dataset：\n",
    "\n",
    "- `SidSFTDataset`\n",
    "- `SidItemFeatDataset`\n",
    "- `FusionSeqRecDataset`\n",
    "- `SFTData`\n",
    "- `TitleHistory2SidSFTDataset`\n",
    "\n",
    "MiniOneRec 这些 Dataset 本来返回的是 `input_ids/labels`。\n",
    "现在改成\n",
    "\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"instruction\": \"...\", \n",
    "    \"input\": \"...\", \n",
    "    \"output\": \"...\"\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c9825a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdb1/sdb1_xiaojinsong/miniconda3/envs/MiniOneRec/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys, json, random\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e0f284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import (\n",
    "    SFTData,\n",
    "    SidSFTDataset,\n",
    "    SidItemFeatDataset,\n",
    "    FusionSeqRecDataset,\n",
    "    TitleHistory2SidSFTDataset,\n",
    ")\n",
    "\n",
    "from sft import TokenExtender\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ef73eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model=\"../llms/Qwen/Qwen3-0.6B\"\n",
    "SFT_DATA_ROOT=\"./data/Amazon2018\"\n",
    "BSZ=32\n",
    "MICRO_BSZ=4\n",
    "\n",
    "seed=42\n",
    "cutoff_len=612\n",
    "sample=-1\n",
    "\n",
    "WANDB_PROJ=\"minionerec\"\n",
    "OUTPUT_DIR=\"output_sft/sft_base\"\n",
    "SID_METHOD=\"rqvae\"\n",
    "\n",
    "category=\"Industrial_and_Scientific\"  # \"Office_Products\"   \n",
    "CATEGORY=category\n",
    "\n",
    "train_file=f\"{SFT_DATA_ROOT}/{CATEGORY}/train/{CATEGORY}_convert.csv\"\n",
    "eval_file=f\"{SFT_DATA_ROOT}/{CATEGORY}/valid/{CATEGORY}_convert.csv\"\n",
    "test_file=f\"{SFT_DATA_ROOT}/{CATEGORY}/test/{CATEGORY}_convert.csv\"\n",
    "info_file=f\"{SFT_DATA_ROOT}/{CATEGORY}/info/{CATEGORY}_convert.txt\"\n",
    "\n",
    "sid_index_path=f\"{SFT_DATA_ROOT}/{CATEGORY}/{CATEGORY}.index.json\"\n",
    "item_meta_path=f\"{SFT_DATA_ROOT}/{CATEGORY}/{CATEGORY}.item.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4487644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading index from ./data/Amazon2018/Industrial_and_Scientific/Industrial_and_Scientific.index.json\n",
      "Adding 590 new tokens to tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model,\n",
    "            # torch_dtype=torch.bfloat16,\n",
    "            dtype=torch.bfloat16,\n",
    "        )\n",
    "\n",
    "if sid_index_path and os.path.exists(sid_index_path):\n",
    "    print(f\"Loading index from {sid_index_path}\")\n",
    "    token_extender = TokenExtender(\n",
    "        data_path=os.path.dirname(sid_index_path),\n",
    "        dataset=os.path.basename(sid_index_path).split('.')[0]\n",
    "    )\n",
    "    new_tokens = token_extender.get_new_tokens()\n",
    "    if new_tokens:\n",
    "        print(f\"Adding {len(new_tokens)} new tokens to tokenizer\")\n",
    "        tokenizer.add_tokens(new_tokens)\n",
    "        model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "482f4855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/33185 [00:00<?, ?it/s]/mnt/sdb1/sdb1_xiaojinsong/tiny-onerec/data.py:518: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  row['history_item_sid'] = eval(row['history_item_sid'])\n",
      "100%|██████████| 33185/33185 [00:16<00:00, 1986.62it/s]\n",
      "100%|██████████| 6531/6531 [00:01<00:00, 3389.03it/s]\n",
      "100%|██████████| 33185/33185 [00:15<00:00, 2077.17it/s]\n",
      "  0%|          | 0/33185 [00:00<?, ?it/s]/mnt/sdb1/sdb1_xiaojinsong/tiny-onerec/data.py:83: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  row['history_item_title'] = eval(row['history_item_title'])\n",
      " 26%|██▋       | 8720/33185 [00:06<00:17, 1367.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 18236/33185 [00:14<00:11, 1300.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 24323/33185 [00:18<00:07, 1258.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33185/33185 [00:26<00:00, 1262.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33185/33185 [00:22<00:00, 1483.71it/s]\n",
      "  0%|          | 0/4148 [00:00<?, ?it/s]/mnt/sdb1/sdb1_xiaojinsong/tiny-onerec/data.py:518: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  row['history_item_sid'] = eval(row['history_item_sid'])\n",
      "100%|██████████| 4148/4148 [00:02<00:00, 1853.09it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data1 = SidSFTDataset(train_file=train_file, tokenizer=tokenizer, max_len=cutoff_len,  sample=sample, seed=seed, category=category)\n",
    "\n",
    "train_data2 = SidItemFeatDataset(item_file=item_meta_path, index_file=sid_index_path, tokenizer=tokenizer, max_len=cutoff_len,  sample=sample, seed=seed, category=category)\n",
    "\n",
    "train_data3 = FusionSeqRecDataset(train_file=train_file, item_file=item_meta_path, index_file=sid_index_path, tokenizer=tokenizer, max_len=cutoff_len, sample=sample, seed=seed, category=category)\n",
    "\n",
    "train_data4 = SFTData(train_file=train_file, tokenizer=tokenizer, max_len=cutoff_len,  sample=sample, seed=seed, category=category)\n",
    "\n",
    "train_data5 = TitleHistory2SidSFTDataset(train_file=train_file, item_file=item_meta_path, index_file=sid_index_path, tokenizer=tokenizer, max_len=cutoff_len, sample=sample, seed=seed, category=category)\n",
    "\n",
    "# train_data = ConcatDataset(train_datasets)\n",
    "val_data = SidSFTDataset(train_file=eval_file, tokenizer=tokenizer, max_len=cutoff_len,  sample=sample, seed=seed, category=category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3622fea7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3823993f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Can you predict the next possible item that the user may expect?', 'input': 'The user has interacted with items <a_40><b_116><c_254>, <a_120><b_233><c_163> in chronological order. Can you predict the next possible item that the user may expect?', 'output': '<a_120><b_135><c_223>'}\n",
      "{'instruction': 'Answer the question about item identification.', 'input': 'What is the title of item \"<a_191><b_110><c_145>\"?', 'output': 'Stanley TRA708T Sharpshooter 1/2-Inch Leg Length Staples, Steel (1000 Count)'}\n",
      "{'instruction': 'Can you recommend the next item for the user based on their interaction history?', 'input': 'The user has sequentially interacted with items <a_40><b_116><c_254>, <a_120><b_233><c_163>. Can you recommend the next item for him? Tell me the title of the item', 'output': 'Elenco  Resistor Substitution Box - RS-400'}\n",
      "{'instruction': \"Write a response that appropriately completes the request. \\nIn relation to the user's recent entertainment with a given Industrial_and_Scientific, it would be appreciated if you could curate a list of Industrial_and_Scientific that might form part of the user's previous gaming history.\", 'input': 'The user has palyed the following Industrial_and_Scientifics before: \"1MHz Function Generator Kit\",\\t\"Set of 10, LM555 LM555CN (IC TIMER) (8 pins DIP)\"', 'output': '\"Elenco  Resistor Substitution Box - RS-400\"'}\n",
      "{'instruction': \"Based on the user's historical interaction with item titles, predict the semantic ID of the next item they may expect.\", 'input': 'The user has interacted with the following Industrial_and_Scientific items in chronological order: \"1MHz Function Generator Kit\", \"Set of 10, LM555 LM555CN (IC TIMER) (8 pins DIP)\". Can you predict the next item the user may expect?', 'output': '<a_120><b_135><c_223>'}\n",
      "{'instruction': 'Can you predict the next possible item that the user may expect?', 'input': 'The user has interacted with items <a_114><b_187><c_20>, <a_49><b_196><c_211>, <a_202><b_124><c_184>, <a_107><b_144><c_103> in chronological order. Can you predict the next possible item that the user may expect?', 'output': '<a_75><b_203><c_10>'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdb1/sdb1_xiaojinsong/tiny-onerec/data.py:83: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  row['history_item_title'] = eval(row['history_item_title'])\n"
     ]
    }
   ],
   "source": [
    "print(train_data1.pre_alpaca(1))\n",
    "print(train_data2.pre_alpaca(1))\n",
    "print(train_data3.pre_alpaca(1))\n",
    "print(train_data4.pre_alpaca(1))\n",
    "print(train_data5.pre_alpaca(1))\n",
    "print(val_data.pre_alpaca(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fac99c3",
   "metadata": {},
   "source": [
    "train_data1 = SidSFTDataset(train_file=train_file, tokenizer=tokenizer, max_len=cutoff_len,  sample=sample, seed=seed, category=category)\n",
    "\n",
    "train_data2 = SidItemFeatDataset(item_file=item_meta_path, index_file=sid_index_path, tokenizer=tokenizer, max_len=cutoff_len,  sample=sample, seed=seed, category=category)\n",
    "\n",
    "train_data3 = FusionSeqRecDataset(train_file=train_file, item_file=item_meta_path, index_file=sid_index_path, tokenizer=tokenizer, max_len=cutoff_len, sample=sample, seed=seed, category=category)\n",
    "\n",
    "train_data4 = SFTData(train_file=train_file, tokenizer=tokenizer, max_len=cutoff_len,  sample=sample, seed=seed, category=category)\n",
    "\n",
    "train_data5 = TitleHistory2SidSFTDataset(train_file=train_file, item_file=item_meta_path, index_file=sid_index_path, tokenizer=tokenizer, max_len=cutoff_len, sample=sample, seed=seed, category=category)\n",
    "\n",
    "val_data = SidSFTDataset(train_file=eval_file, tokenizer=tokenizer, max_len=cutoff_len,  sample=sample, seed=seed, category=category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393261e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/33185 [00:00<?, ?it/s]/mnt/sdb1/sdb1_xiaojinsong/tiny-onerec/data.py:518: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  row['history_item_sid'] = eval(row['history_item_sid'])\n",
      "100%|██████████| 33185/33185 [00:03<00:00, 9301.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6531/6531 [00:00<00:00, 1006244.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33185/33185 [00:01<00:00, 16694.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/33185 [00:00<?, ?it/s]/mnt/sdb1/sdb1_xiaojinsong/tiny-onerec/data.py:83: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  row['history_item_title'] = eval(row['history_item_title'])\n",
      "100%|██████████| 33185/33185 [00:04<00:00, 7919.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33185/33185 [00:02<00:00, 15940.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4148/4148 [00:00<00:00, 8924.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_sid_sft = train_data1.get_alpaca()\n",
    "print(len(train_sid_sft))\n",
    "\n",
    "train_sid_item_feat = train_data2.get_alpaca()\n",
    "print(len(train_sid_item_feat))\n",
    "\n",
    "train_fusion_seq_rec = train_data3.get_alpaca()\n",
    "print(len(train_fusion_seq_rec))\n",
    "\n",
    "train_sft = train_data4.get_alpaca()\n",
    "print(len(train_sft))\n",
    "\n",
    "train_title_history2sid_sft = train_data5.get_alpaca()\n",
    "print(len(train_title_history2sid_sft))\n",
    "\n",
    "# 139271\n",
    "\n",
    "val_sid_sft = val_data.get_alpaca()\n",
    "print(len(val_sid_sft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ac3a3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: plug your datasets here\n",
    "DATASETS_TRAIN = {\n",
    "    \"train_sid_sft\": train_sid_sft,\n",
    "    \"train_sid_item_feat\": train_sid_item_feat,\n",
    "    \"train_fusion_seq_rec\": train_fusion_seq_rec,\n",
    "    \"train_sft\": train_sft,\n",
    "    \"train_title_history2sid_sft\": train_title_history2sid_sft,\n",
    "}\n",
    "\n",
    "DATASETS_EVAL = {\n",
    "    \"val_sid_sft\": val_sid_sft,\n",
    "}\n",
    "\n",
    "LIMIT_PER_DATASET = None  # e.g. 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1411c237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] train_sid_sft: -> sft_data/train_sid_sft.jsonl\n",
      "[OK] train_sid_item_feat: -> sft_data/train_sid_item_feat.jsonl\n",
      "[OK] train_fusion_seq_rec: -> sft_data/train_fusion_seq_rec.jsonl\n",
      "[OK] train_sft: -> sft_data/train_sft.jsonl\n",
      "[OK] train_title_history2sid_sft: -> sft_data/train_title_history2sid_sft.jsonl\n",
      "[OK] val_sid_sft: -> sft_data/val_sid_sft.jsonl\n"
     ]
    }
   ],
   "source": [
    "OUT_DIR = Path(\"./sft_data\")   # <<< 改这里\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "saved_paths = {}\n",
    "\n",
    "# train\n",
    "for name, ds in DATASETS_TRAIN.items():\n",
    "    out_path = OUT_DIR / f\"{name}.jsonl\"\n",
    "\n",
    "    print(f\"[OK] {name}: -> {out_path}\")\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for ex in ds:\n",
    "            ex[\"instruction\"] = ex[\"instruction\"].strip()\n",
    "            ex[\"input\"] = ex[\"input\"].strip()\n",
    "            ex[\"output\"] = ex[\"output\"].strip()\n",
    "            f.write(json.dumps(ex, ensure_ascii=False) + \"\\n\")\n",
    "# eval\n",
    "for name, ds in DATASETS_EVAL.items():\n",
    "    out_path = OUT_DIR / f\"{name}.jsonl\"\n",
    "    print(f\"[OK] {name}: -> {out_path}\")\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for ex in ds:\n",
    "            ex[\"instruction\"] = ex[\"instruction\"].strip()\n",
    "            ex[\"input\"] = ex[\"input\"].strip()\n",
    "            ex[\"output\"] = ex[\"output\"].strip()\n",
    "            f.write(json.dumps(ex, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf54f44",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c778c171",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MiniOneRec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
