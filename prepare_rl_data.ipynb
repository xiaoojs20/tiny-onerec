{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98e35ca7",
   "metadata": {},
   "source": [
    "# 构造 `VeRL` 可用的 RL 数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ed906",
   "metadata": {},
   "source": [
    "RL 训练时用到的 3 个 Dataset：\n",
    "\n",
    "- `SidDataset`\n",
    "- `RLTitle2SidDataset`\n",
    "- `RLSeqTitle2SidDataset`\n",
    "\n",
    "MiniOneRec 这些 Dataset 本来返回的是 `input_ids/labels`。\n",
    "现在改成\n",
    "\n",
    "目标输出：\n",
    "- `train.parquet`\n",
    "- `val.parquet`\n",
    "\n",
    "每条样本（row）建议字段（schema）如下：\n",
    "- `data_source`: string（子任务名，如 `sid` / `title2sid` / `seq_title2sid`）  \n",
    "- `prompt`: list[dict]（HF chat template 消息列表：`[{role, content}, ...]`）  \n",
    "- `ability`: string（任务大类，如 `rec`）  \n",
    "- `reward_model`: dict（至少包含 `ground_truth`；也可带 `style` 等）  \n",
    "- `extra_info`: dict（你想透传到 reward function 的额外信息）\n",
    "\n",
    "> 在 veRL 的自定义 reward 中，会拿到：`data_source, solution_str, ground_truth, extra_info`，因此务必把 reward 所需信息放进 `reward_model.ground_truth` 或 `extra_info`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9825a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, random\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0f284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import (\n",
    "    SidDataset, \n",
    "    RLTitle2SidDataset, \n",
    "    RLSeqTitle2SidDataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef73eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model=\"../llms/Qwen/Qwen3-0.6B\"\n",
    "SFT_DATA_ROOT=\"./data/Amazon2018\"\n",
    "BSZ=32\n",
    "MICRO_BSZ=4\n",
    "\n",
    "seed=42\n",
    "cutoff_len=612\n",
    "sample=-1\n",
    "\n",
    "category=\"Industrial_and_Scientific\"  # \"Office_Products\"   \n",
    "CATEGORY=category\n",
    "\n",
    "category_dict = {\"Industrial_and_Scientific\": \"industrial and scientific items\", \"Office_Products\": \"office products\", \"Toys_and_Games\": \"toys and games\", \"Sports\": \"sports and outdoors\", \"Books\": \"books\"}\n",
    "\n",
    "train_file=f\"{SFT_DATA_ROOT}/{CATEGORY}/train/{CATEGORY}_convert.csv\"\n",
    "eval_file=f\"{SFT_DATA_ROOT}/{CATEGORY}/valid/{CATEGORY}_convert.csv\"\n",
    "test_file=f\"{SFT_DATA_ROOT}/{CATEGORY}/test/{CATEGORY}_convert.csv\"\n",
    "info_file=f\"{SFT_DATA_ROOT}/{CATEGORY}/info/{CATEGORY}_convert.txt\"\n",
    "\n",
    "sid_index_path=f\"{SFT_DATA_ROOT}/{CATEGORY}/{CATEGORY}.index.json\"\n",
    "item_meta_path=f\"{SFT_DATA_ROOT}/{CATEGORY}/{CATEGORY}.item.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482f4855",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data1 = SidDataset(train_file, category=category_dict[category], sample=sample)\n",
    "\n",
    "train_data2 = RLTitle2SidDataset(item_file=item_meta_path, index_file=sid_index_path, category=category_dict[category], sample=sample)\n",
    "\n",
    "train_data3 = RLSeqTitle2SidDataset(train_file, category=category_dict[category], sample=10000)\n",
    "\n",
    "# train_data = ConcatDataset(train_datasets)\n",
    "eval_data = SidDataset(eval_file, category=category_dict[category], sample=sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3823993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data1.pre_alpaca(1))\n",
    "print(train_data2.pre_alpaca(1))\n",
    "print(train_data3.pre_alpaca(1))\n",
    "print(eval_data.pre_alpaca(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393261e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rl_sid = train_data1.get_alpaca()\n",
    "print(len(train_rl_sid))\n",
    "\n",
    "train_rl_title2sid = train_data2.get_alpaca()\n",
    "print(len(train_rl_title2sid))\n",
    "\n",
    "train_rl_seqtitle2sid = train_data3.get_alpaca()\n",
    "print(len(train_rl_seqtitle2sid))\n",
    "\n",
    "eval_rl_sid = eval_data.get_alpaca()\n",
    "print(len(eval_rl_sid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac3a3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plug datasets here\n",
    "DATASETS_TRAIN = {\n",
    "    \"train_rl_sid\": train_rl_sid,\n",
    "    \"train_rl_title2sid\": train_rl_title2sid,\n",
    "    \"train_rl_seqtitle2sid\": train_rl_seqtitle2sid,\n",
    "}\n",
    "\n",
    "DATASETS_EVAL = {\n",
    "    \"eval_rl_sid\": eval_rl_sid,\n",
    "}\n",
    "\n",
    "LIMIT_PER_DATASET = None  # e.g. 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1411c237",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = Path(\"./rl_data\")   # <<< 改这里\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "saved_paths = {}\n",
    "\n",
    "# train\n",
    "for name, ds in DATASETS_TRAIN.items():\n",
    "    out_path = OUT_DIR / f\"{name}.jsonl\"\n",
    "\n",
    "    print(f\"[OK] {name}: -> {out_path}\")\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for ex in ds:\n",
    "            ex[\"instruction\"] = ex[\"instruction\"].strip()\n",
    "            ex[\"input\"] = ex[\"input\"].strip()\n",
    "            ex[\"output\"] = ex[\"output\"].strip()\n",
    "            f.write(json.dumps(ex, ensure_ascii=False) + \"\\n\")\n",
    "# eval\n",
    "for name, ds in DATASETS_EVAL.items():\n",
    "    out_path = OUT_DIR / f\"{name}.jsonl\"\n",
    "    print(f\"[OK] {name}: -> {out_path}\")\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for ex in ds:\n",
    "            ex[\"instruction\"] = ex[\"instruction\"].strip()\n",
    "            ex[\"input\"] = ex[\"input\"].strip()\n",
    "            ex[\"output\"] = ex[\"output\"].strip()\n",
    "            f.write(json.dumps(ex, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf54f44",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c778c171",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MiniOneRec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
